{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# vLLM Tool Call Parser Comparison\n\nThis notebook demonstrates:\n1. **Pydantic-validated tool call models** with schema validation\n2. **Three parser implementations** (Regex, Incremental, State Machine)\n3. **vLLM comparison** - custom parsers vs native tool parsing\n4. **Structured output approaches** - Post-hoc parsing vs Constrained decoding (Outlines, XGrammar)\n\n## Key Features for vLLM Tool Calling\n- OpenAI Chat Completions API compatibility\n- Incremental/streaming parsing for early tool call detection\n- Error recovery from malformed LLM output\n- JSON Schema validation\n\n## Requirements\n- Google Colab with GPU runtime (T4 recommended)\n- ~8GB GPU memory for 7B models"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Install vLLM and dependencies\n",
    "!pip install -q vllm openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone the Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo\n",
    "!git clone https://github.com/shravsssss/vLLM-Tool-Call-Parser.git\n",
    "%cd vLLM-Tool-Call-Parser\n",
    "\n",
    "# Install requirements\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Pydantic Models for Tool Calls\n# These models provide validation compatible with OpenAI Chat Completions API\n\nimport json\nimport re\nfrom typing import Any\nfrom pydantic import BaseModel, Field, field_validator, model_validator\n\nclass ToolCall(BaseModel):\n    \"\"\"Pydantic-validated tool call model.\n    \n    Features:\n    - Function name validation (must be valid identifier)\n    - Auto-parsing of JSON string arguments\n    - OpenAI API format compatibility\n    - JSON Schema validation\n    \"\"\"\n    \n    id: str | None = Field(default=None, description=\"Tool call ID (e.g., call_abc123)\")\n    name: str = Field(..., min_length=1, max_length=256)\n    arguments: dict[str, Any] = Field(default_factory=dict)\n    \n    @field_validator(\"name\")\n    @classmethod\n    def validate_function_name(cls, v: str) -> str:\n        \"\"\"Ensure function name is a valid Python identifier.\"\"\"\n        v = v.strip()\n        if not re.match(r\"^[a-zA-Z_][a-zA-Z0-9_]*$\", v):\n            raise ValueError(f\"Invalid function name '{v}'\")\n        return v\n    \n    @field_validator(\"arguments\", mode=\"before\")\n    @classmethod  \n    def parse_arguments(cls, v: Any) -> dict[str, Any]:\n        \"\"\"Auto-parse JSON string arguments (OpenAI format).\"\"\"\n        if isinstance(v, str):\n            return json.loads(v) if v.strip() else {}\n        return v or {}\n    \n    def to_openai_format(self) -> dict[str, Any]:\n        \"\"\"Convert to OpenAI tool_calls format.\"\"\"\n        return {\n            \"id\": self.id,\n            \"type\": \"function\", \n            \"function\": {\n                \"name\": self.name,\n                \"arguments\": json.dumps(self.arguments),\n            }\n        }\n    \n    def matches_schema(self, schema: dict[str, Any]) -> bool:\n        \"\"\"Validate arguments against JSON Schema.\"\"\"\n        required = schema.get(\"required\", [])\n        for field in required:\n            if field not in self.arguments:\n                return False\n        return True\n\n# Demo the Pydantic model\nprint(\"=\" * 60)\nprint(\"PYDANTIC TOOL CALL VALIDATION DEMO\")\nprint(\"=\" * 60)\n\n# Valid tool call\ncall = ToolCall(name=\"get_weather\", arguments={\"city\": \"NYC\", \"unit\": \"celsius\"})\nprint(f\"\\n1. Valid ToolCall: {call}\")\nprint(f\"   OpenAI format: {call.to_openai_format()}\")\n\n# Auto-parse JSON string (OpenAI API sends arguments as string)\ncall2 = ToolCall(name=\"search\", arguments='{\"query\": \"python\", \"limit\": 10}')\nprint(f\"\\n2. Auto-parsed JSON string: {call2.arguments}\")\n\n# Schema validation\nschema = {\"required\": [\"city\"], \"properties\": {\"city\": {\"type\": \"string\"}}}\nprint(f\"\\n3. Schema validation: {call.matches_schema(schema)}\")\n\n# Validation error demo\ntry:\n    bad_call = ToolCall(name=\"123invalid\", arguments={})\nexcept Exception as e:\n    print(f\"\\n4. Validation error (expected): {e}\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 2.5: Pydantic Models Demo\n\nThis project uses **Pydantic** for robust tool call validation - a key skill for vLLM development.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# If cloning fails (private repo), install directly\n!pip install -q pydantic>=2.0 openai>=1.0.0\n\n# Create necessary directories and files inline\nimport os\nos.makedirs(\"src/parser_benchmark/models\", exist_ok=True)\nos.makedirs(\"src/parser_benchmark/parsers\", exist_ok=True)\n\nprint(\"Setup complete! You can either:\")\nprint(\"1. Clone the repo if it's public\")\nprint(\"2. Or copy the parser code from the cells below\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Start vLLM Server\n",
    "\n",
    "Choose your model and parser type. Available parsers:\n",
    "- `hermes` - For Hermes/Qwen models (XML-wrapped JSON)\n",
    "- `llama3_json` - For Llama 3.x models\n",
    "- `mistral` - For Mistral models\n",
    "- `granite` - For IBM Granite models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Small model for Colab free tier\n",
    "# MODEL = \"Qwen/Qwen2.5-7B-Instruct\"  # Larger model (needs more GPU RAM)\n",
    "\n",
    "PARSER = \"hermes\"  # Options: hermes, llama3_json, mistral, granite\n",
    "\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Parser: {PARSER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start vLLM server in background\n",
    "vllm_process = subprocess.Popen(\n",
    "    [\n",
    "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\", MODEL,\n",
    "        \"--tool-call-parser\", PARSER,\n",
    "        \"--enable-auto-tool-choice\",\n",
    "        \"--port\", \"8000\",\n",
    "        \"--max-model-len\", \"4096\",\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "\n",
    "print(\"Starting vLLM server...\")\n",
    "print(\"This may take 1-2 minutes to load the model.\")\n",
    "\n",
    "# Wait for server to be ready\n",
    "import requests\n",
    "for i in range(120):\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:8000/health\")\n",
    "        if response.status_code == 200:\n",
    "            print(f\"\\nvLLM server is ready! (took {i+1} seconds)\")\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(1)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Still loading... ({i}s)\")\n",
    "else:\n",
    "    print(\"Timeout waiting for server. Check logs below:\")\n",
    "    print(vllm_process.stdout.read().decode()[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Connect to vLLM\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\n",
    "\n",
    "# Test with a simple tool call\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\"type\": \"string\", \"description\": \"City name\"},\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"city\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=client.models.list().data[0].id,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}],\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Test Response:\")\n",
    "print(f\"Content: {response.choices[0].message.content}\")\n",
    "print(f\"Tool Calls: {response.choices[0].message.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Full Comparison\n",
    "\n",
    "This runs comprehensive tests comparing your custom parsers against vLLM's native parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "from parser_benchmark.vllm_comparison import (\n",
    "    VLLMConfig,\n",
    "    VLLMComparisonRunner,\n",
    "    DEFAULT_TEST_PROMPTS,\n",
    ")\n",
    "\n",
    "# Create config and runner\n",
    "config = VLLMConfig.local(port=8000, parser=PARSER)\n",
    "runner = VLLMComparisonRunner(config)\n",
    "\n",
    "# Check connection\n",
    "connected, message = runner.check_connection()\n",
    "print(message)\n",
    "\n",
    "if not connected:\n",
    "    print(\"Error: vLLM server not reachable. Check the server logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full comparison\n",
    "print(\"Running full comparison...\")\n",
    "print(f\"Testing with {len(DEFAULT_TEST_PROMPTS)} prompts\")\n",
    "\n",
    "report = runner.run_full_comparison(\n",
    "    prompts=DEFAULT_TEST_PROMPTS,\n",
    "    test_streaming=True,\n",
    "    test_error_recovery=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ACCURACY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Parser':<25} {'Accuracy':>10}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'vLLM Native':<25} {report.vllm_accuracy:>9.1f}%\")\n",
    "for name, acc in report.accuracy_scores.items():\n",
    "    print(f\"{name:<25} {acc:>9.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LATENCY COMPARISON (parsing only)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Parser':<25} {'Avg Latency':>12}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'vLLM Native':<25} {report.vllm_avg_latency_ms:>10.2f} ms\")\n",
    "for name, lat in report.avg_latency_ms.items():\n",
    "    speedup = report.vllm_avg_latency_ms / lat if lat > 0 else 0\n",
    "    print(f\"{name:<25} {lat:>10.2f} ms ({speedup:.1f}x faster)\")\n",
    "\n",
    "if report.streaming_advantage:\n",
    "    sa = report.streaming_advantage\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STREAMING ADVANTAGE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nvLLM waits for complete response: {sa.vllm_total_time_ms:.1f} ms\")\n",
    "    print(f\"Incremental parser first detection: {sa.incremental_first_call_ms:.1f} ms\")\n",
    "    print(f\"\\nAdvantage: {sa.advantage_ms:.1f} ms ({sa.advantage_percent:.1f}% earlier detection)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ERROR RECOVERY WINS\")\n",
    "print(\"=\"*60)\n",
    "for parser, wins in report.error_recovery_summary.items():\n",
    "    print(f\"  {parser}: {wins} cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results for Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Save results to JSON\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"vllm_comparison_{timestamp}.json\"\n",
    "\n",
    "runner.save_results(report, output_file)\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "from google.colab import files\n",
    "files.download(output_file)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"  \n",
    "1. Download the JSON file (should start automatically)\n",
    "2. Go to the HuggingFace Space dashboard\n",
    "3. Click on 'vLLM Comparison' tab\n",
    "4. Upload the JSON file\n",
    "5. See your comparison charts!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Test with Different Parsers\n",
    "\n",
    "Restart vLLM with a different `--tool-call-parser` to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Summary: Skills Demonstrated\n\nThis notebook demonstrates key skills for vLLM tool calling development:\n\n| Skill | Demonstrated |\n|-------|--------------|\n| **Python + Pydantic** | ToolCall model with validators |\n| **OpenAI API Compatibility** | `to_openai_format()`, Chat Completions |\n| **Incremental Parsing** | Early tool call detection in streaming |\n| **Outlines/XGrammar Knowledge** | Comparison with constrained decoding |\n| **vLLM Tool Parsers** | Testing hermes, llama3_json, mistral |\n| **Error Recovery** | Handling malformed LLM output |\n| **JSON Schema Validation** | `matches_schema()` method |\n\n**Links:**\n- [HuggingFace Dashboard](https://huggingface.co/spaces/sravyayepuri/tool-call-parser-benchmark)\n- [Outlines](https://github.com/outlines-dev/outlines)\n- [XGrammar](https://github.com/mlc-ai/xgrammar)\n- [vLLM Guided Decoding](https://docs.vllm.ai/en/latest/features/structured_outputs.html)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# vLLM Guided Decoding Example (Constrained Decoding)\n# This shows how vLLM can use BOTH approaches\n\nprint(\"\"\"\nvLLM SUPPORTS BOTH APPROACHES:\n\n1. POST-HOC PARSING (what we tested above):\n   python -m vllm.entrypoints.openai.api_server \\\\\n       --model Qwen/Qwen2.5-7B-Instruct \\\\\n       --tool-call-parser hermes \\\\\n       --enable-auto-tool-choice\n\n2. CONSTRAINED DECODING (guaranteed valid JSON):\n\"\"\")\n\n# Example of vLLM guided decoding (requires vLLM Python API)\nguided_decoding_example = '''\nfrom vllm import LLM, SamplingParams\n\nllm = LLM(model=\"Qwen/Qwen2.5-7B-Instruct\")\n\n# Define JSON Schema for tool calls\ntool_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"arguments\": {\"type\": \"object\"}\n    },\n    \"required\": [\"name\"]\n}\n\n# Use guided decoding - output GUARANTEED to match schema\nsampling_params = SamplingParams(\n    guided_decoding_backend=\"xgrammar\",  # or \"outlines\"\n    guided_json=tool_schema,\n    max_tokens=256\n)\n\noutputs = llm.generate([\"Call a function to get weather\"], sampling_params)\n# outputs[0].outputs[0].text is ALWAYS valid JSON\n'''\n\nprint(guided_decoding_example)\n\nprint(\"\"\"\nHYBRID APPROACH (Recommended for Production):\n  1. Use guided decoding to ensure valid JSON structure\n  2. Use custom parser to extract and validate against app schema\n  3. Best of both worlds: guaranteed structure + fast validation\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Structured Output Approaches Comparison\n\nprint(\"\"\"\n╔══════════════════════════════════════════════════════════════════════════════╗\n║              STRUCTURED OUTPUT: TWO APPROACHES                                ║\n╠══════════════════════════════════════════════════════════════════════════════╣\n║                                                                               ║\n║  1. POST-HOC PARSING (This Project)                                           ║\n║     ├─ Parse LLM output AFTER generation                                      ║\n║     ├─ Works with ANY LLM API (OpenAI, Anthropic, vLLM)                       ║\n║     ├─ Supports streaming with early detection                                ║\n║     ├─ Can recover from malformed output                                      ║\n║     └─ Zero generation overhead                                               ║\n║                                                                               ║\n║  2. CONSTRAINED DECODING (Outlines, XGrammar)                                 ║\n║     ├─ Guide generation at LOGIT level                                        ║\n║     ├─ Mask invalid tokens during sampling                                    ║\n║     ├─ 100% guaranteed valid output                                           ║\n║     ├─ Requires inference engine integration                                  ║\n║     └─ 2-15% generation overhead                                              ║\n║                                                                               ║\n╚══════════════════════════════════════════════════════════════════════════════╝\n\nCOMPARISON TABLE:\n\"\"\")\n\ncomparison = \"\"\"\n┌────────────────────────┬───────────────────────┬─────────────────────────────┐\n│ Aspect                 │ Post-hoc Parsing      │ Constrained Decoding        │\n├────────────────────────┼───────────────────────┼─────────────────────────────┤\n│ When it runs           │ After generation      │ During generation           │\n│ Guarantees valid JSON  │ No (recovers errors)  │ Yes (100%)                  │\n│ Latency overhead       │ <0.1ms                │ 2-15% generation time       │\n│ Works with any LLM     │ Yes                   │ Requires integration        │\n│ Streaming support      │ Yes (early detection) │ Limited                     │\n│ Error recovery         │ Yes                   │ N/A (no errors)             │\n└────────────────────────┴───────────────────────┴─────────────────────────────┘\n\"\"\"\nprint(comparison)\n\nprint(\"\"\"\nKEY LIBRARIES:\n\nOUTLINES (github.com/outlines-dev/outlines):\n  - Uses Finite State Machines from JSON Schema\n  - Masks invalid logits during sampling\n  - Example:\n    generator = outlines.generate.json(model, schema)\n    result = generator(prompt)  # Always valid JSON\n\nXGRAMMAR (github.com/mlc-ai/xgrammar):\n  - Compiles grammars to token masks\n  - Optimized for batch inference\n  - Supports JSON Schema, regex, EBNF\n\nvLLM INTEGRATION:\n  - Post-hoc: --tool-call-parser hermes/llama3_json/mistral\n  - Constrained: guided_decoding_backend=\"outlines\" or \"xgrammar\"\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Structured Output: Parsing vs Constrained Decoding\n\nThis section compares two approaches to structured LLM output:\n\n| Approach | Description | Libraries |\n|----------|-------------|-----------|\n| **Post-hoc Parsing** | Parse output after generation | This project, vLLM parsers |\n| **Constrained Decoding** | Guide generation at logit level | Outlines, XGrammar, Guidance |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restart_vllm_with_parser(parser_name: str):\n",
    "    \"\"\"Restart vLLM with a different parser.\"\"\"\n",
    "    global vllm_process, PARSER\n",
    "    \n",
    "    # Kill existing process\n",
    "    if vllm_process:\n",
    "        vllm_process.terminate()\n",
    "        vllm_process.wait()\n",
    "        time.sleep(2)\n",
    "    \n",
    "    PARSER = parser_name\n",
    "    \n",
    "    # Start new process\n",
    "    vllm_process = subprocess.Popen(\n",
    "        [\n",
    "            \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "            \"--model\", MODEL,\n",
    "            \"--tool-call-parser\", parser_name,\n",
    "            \"--enable-auto-tool-choice\",\n",
    "            \"--port\", \"8000\",\n",
    "            \"--max-model-len\", \"4096\",\n",
    "        ],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "    )\n",
    "    \n",
    "    print(f\"Restarting with {parser_name} parser...\")\n",
    "    \n",
    "    # Wait for ready\n",
    "    for i in range(120):\n",
    "        try:\n",
    "            response = requests.get(\"http://localhost:8000/health\")\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Ready with {parser_name} parser!\")\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(\"Timeout!\")\n",
    "    return False\n",
    "\n",
    "# Example: Test with llama3_json parser\n",
    "# restart_vllm_with_parser(\"llama3_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop vLLM server\n",
    "if vllm_process:\n",
    "    vllm_process.terminate()\n",
    "    vllm_process.wait()\n",
    "    print(\"vLLM server stopped.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}